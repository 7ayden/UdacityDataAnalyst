{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load poi_id.py\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "from time import time\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df= pd.DataFrame(data_dict)\n",
    "POI_count = 0\n",
    "name_list = data_dict.keys()\n",
    "for person in name_list:\n",
    "    POI_count += data_dict[person]['poi']    \n",
    "print('Number of flagged Persons of Interest: %d' % POI_count)\n",
    "print('Number of people without POI flag: %d' % (len(name_list) - POI_count))\n",
    "print('Total Number of people: %d' % len(df.keys()))\n",
    "#print(\"Number of features per person: %d\"%len(list(data_dict[person].count))\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "persons = pd.Series(list(data_dict.keys()))\n",
    "\n",
    "print df.head()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "total_index = data_dict.keys().index(\"TOTAL\")\n",
    "print \"There are \", (total_index), \"Total index\"\n",
    "travel_index = data_dict.keys().index(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "print \"There are \", (travel_index), \"travel agency in the park\"\n",
    "print('Number of flagged Persons of Interest: %d' % len(data_dict['SKILLING JEFFREY K'].keys()))\n",
    "print \"LOCKHART EUGENE E \", data_dict['LOCKHART EUGENE E']\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "NA_salary = []\n",
    "for x in data_dict.values():\n",
    "    if x['salary'] != 'NaN':\n",
    "        NA_salary.append(x['salary'])\n",
    "len(NA_salary),\"have a quantified salary from this dataset\"\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "email_array = []\n",
    "for x in data_dict.values():\n",
    "    if x['email_address'] == 'NaN':\n",
    "        email_array.append(x['email_address'])\n",
    "        \n",
    "print len(email_array), \"has email address\"\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "# Convert to numpy nan\n",
    "df.replace(to_replace='NaN', value=np.nan, inplace=True)\n",
    "\n",
    "# Count number of NaN's for columns\n",
    "print df.isnull().sum()\n",
    "\n",
    "# DataFrame dimeansion\n",
    "print df.shape\n",
    "\n",
    "nandf = df.isnull().sum()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "features = [\"salary\", \"bonus\"]\n",
    "#data_dict.pop('TOTAL', 0)\n",
    "data = featureFormat(data_dict, features)\n",
    "### plot features\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter( salary, bonus )\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL', 0)\n",
    "data = featureFormat(data_dict, features_list) \n",
    "\n",
    "### remove NAN's from dataset\n",
    "outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['salary']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    outliers.append((key, int(val)))\n",
    "\n",
    "outliers_final = (sorted(outliers,key=lambda x:x[1],reverse=True)[:4])\n",
    "### print top 4 salaries\n",
    "print outliers_final\n",
    "\n",
    "for name in outliers_final:\n",
    "    if data_dict[name[0]]['poi'] ==1:\n",
    "        print name[0]\n",
    "        \n",
    "import csv\n",
    "df.to_csv('enron_data.csv')\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "def dict_to_list(key,normalizer):\n",
    "    new_list=[]\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n",
    "            new_list.append(0.)\n",
    "        elif data_dict[i][key]>=0:\n",
    "            new_list.append(float(data_dict[i][key])/float(data_dict[i][normalizer]))\n",
    "    return new_list\n",
    "\n",
    "### create two lists of new features\n",
    "poi_sender_fract=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\n",
    "poi_recipient_fract=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\n",
    "\n",
    "### insert new features into data_dict\n",
    "count=0\n",
    "for i in data_dict:\n",
    "    data_dict[i][\"poi_sender_fract\"]=poi_sender_fract[count]\n",
    "    data_dict[i][\"poi_recipient_fract\"]=poi_recipient_fract[count]\n",
    "    count +=1\n",
    "\n",
    "    \n",
    "features_list = [\"poi\", \"poi_sender_fract\", \"poi_recipient_fract\"]    \n",
    "    ### store to my_dataset for easy export below\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "### these two lines extract the features specified in features_list\n",
    "### and extract them from data_dict, returning a numpy array\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "\n",
    "### plot new features\n",
    "for point in data:\n",
    "    from_poi = point[1]\n",
    "    to_poi = point[2]\n",
    "    plt.scatter( from_poi, to_poi )\n",
    "    if point[0] == 1:\n",
    "        plt.scatter(from_poi, to_poi, color=\"r\", marker=\"*\")\n",
    "plt.xlabel(\"fraction of emails this person gets from poi\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "\n",
    "my_features = features_list + [\n",
    "'shared_receipt_with_poi',\n",
    "'expenses',\n",
    "'loan_advances',\n",
    "'long_term_incentive',\n",
    "'other',\n",
    " 'restricted_stock',\n",
    "'restricted_stock_deferred',\n",
    "'deferral_payments',\n",
    " 'deferred_income',\n",
    "'salary',\n",
    "'total_stock_value',\n",
    "'exercised_stock_options',\n",
    "'total_payments',\n",
    "'bonus']\n",
    "\n",
    "\n",
    "features_list = [\"poi\", \"salary\", \"bonus\", \"poi_sender_fract\", \"poi_recipient_fract\",\n",
    "                 'deferral_payments', 'total_payments', 'loan_advances', 'restricted_stock_deferred',\n",
    "                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options',\n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\n",
    "data = featureFormat(data_dict, features_list)\n",
    "\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "k_best = SelectKBest(k=5)\n",
    "k_best.fit(features, labels)\n",
    "\n",
    "results_list = zip(k_best.get_support(), my_features[1:], k_best.scores_)\n",
    "results_list = sorted(results_list, key=lambda x: x[2], reverse=True)\n",
    "print \"K-best features:\", results_list\n",
    "\n",
    "## 5 best features chosen by SelectKBest\n",
    "my_features = features_list + ['exercised_stock_options',\n",
    "'total_stock_value',\n",
    "'bonus',\n",
    " 'salary',\n",
    "'poi_sender_fract']\n",
    "\n",
    "data = featureFormat(my_dataset, my_features, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "features_list = [\"poi\", \"salary\", \"bonus\", \"poi_sender_fract\", \"poi_recipient_fract\",\n",
    "                 'deferral_payments', 'total_payments', 'loan_advances', 'restricted_stock_deferred',\n",
    "                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options',\n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\n",
    "data = featureFormat(data_dict, features_list)\n",
    "\n",
    "### split into labels and features (this line assumes that the first\n",
    "### feature in the array is the label, which is why \"poi\" must always\n",
    "### be first in features_list\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### split data into training and testing datasets\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "pred= clf.predict(features_test)\n",
    "print 'accuracy', score\n",
    "\n",
    "print \"Decision tree algorithm time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "import numpy as np\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print 'Feature Ranking: '\n",
    "for i in range(16):\n",
    "    print \"{} feature {} ({})\".format(i+1,features_list[i+1],importances[indices[i]])\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "features_list = [\"poi\", \"poi_sender_fract\", \"poi_recipient_fract\", \"shared_receipt_with_poi\"]\n",
    "### try Naive Bayes for prediction\n",
    "t0 = time()\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred,labels_test)\n",
    "print accuracy\n",
    "\n",
    "print \"NB algorithm time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "### use manual tuning parameter min_samples_split\n",
    "clf = DecisionTreeClassifier(min_samples_split=5)\n",
    "print clf\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "### features_list is a list of strings, each of which is a feature name\n",
    "### first feature must be \"poi\", as this will be singled out as the label\n",
    "#features_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\", 'shared_receipt_with_poi']\n",
    "features_list = [\"poi\", \"poi_sender_fract\", \"poi_recipient_fract\", \"shared_receipt_with_poi\"]\n",
    "\n",
    "### store to my_dataset for easy export below\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "### these two lines extract the features specified in features_list\n",
    "### and extract them from data_dict, returning a numpy array\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "\n",
    "\n",
    "### split into labels and features (this line assumes that the first\n",
    "### feature in the array is the label, which is why \"poi\" must always\n",
    "### be first in features_list\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "### machine learning goes here!\n",
    "### please name your classifier clf for easy export below\n",
    "\n",
    "### deploying feature selection\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "### use KFold for split and validate algorithm\n",
    "from sklearn.cross_validation import KFold\n",
    "kf=KFold(len(labels),3)\n",
    "for train_indices, test_indices in kf:\n",
    "    #make training and testing sets\n",
    "    features_train= [features[ii] for ii in train_indices]\n",
    "    features_test= [features[ii] for ii in test_indices]\n",
    "    labels_train=[labels[ii] for ii in train_indices]\n",
    "    labels_test=[labels[ii] for ii in test_indices]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "print 'accuracy before tuning ', score\n",
    "\n",
    "print \"Decision tree algorithm time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "\n",
    "### use manual tuning parameter min_samples_split\n",
    "t0 = time()\n",
    "clf = DecisionTreeClassifier(min_samples_split=5)\n",
    "clf = clf.fit(features_train,labels_train)\n",
    "pred= clf.predict(features_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "acc=accuracy_score(labels_test, pred)\n",
    "\n",
    "print \"Validating algorithm:\"\n",
    "print \"accuracy after tuning = \", acc\n",
    "\n",
    "# function for calculation ratio of true positives\n",
    "# out of all positives (true + false)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print 'precision = ', precision_score(labels_test,pred)\n",
    "\n",
    "# function for calculation ratio of true positives\n",
    "# out of true positives and false negatives\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print 'recall = ', recall_score(labels_test,pred)\n",
    "\n",
    "\n",
    "### dump your classifier, dataset and features_list so\n",
    "### anyone can run/check your results\n",
    "pickle.dump(clf, open(\"my_classifier.pkl\", \"w\") )\n",
    "pickle.dump(data_dict, open(\"my_dataset.pkl\", \"w\") )\n",
    "pickle.dump(features_list, open(\"my_feature_list.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    #Added min max scaler here\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    features = min_max_scaler.fit_transform(features)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_positives += 1\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    pickle.dump(clf, open(CLF_PICKLE_FILENAME, \"w\") )\n",
    "    pickle.dump(dataset, open(DATASET_PICKLE_FILENAME, \"w\") )\n",
    "    pickle.dump(feature_list, open(FEATURE_LIST_FILENAME, \"w\") )\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    clf = pickle.load(open(CLF_PICKLE_FILENAME, \"r\") )\n",
    "    dataset = pickle.load(open(DATASET_PICKLE_FILENAME, \"r\") )\n",
    "    feature_list = pickle.load(open(FEATURE_LIST_FILENAME, \"r\"))\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test =     train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test =     train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
