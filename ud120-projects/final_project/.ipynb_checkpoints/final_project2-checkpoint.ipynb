{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import pandas as pd\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "\n",
    "### features_list is a list of strings, each of which is a feature name\n",
    "### first feature must be \"poi\", as this will be singled out as the label\n",
    "features_list = [\"poi\"]\n",
    "\n",
    "### load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are set([21]) features\n"
     ]
    }
   ],
   "source": [
    "print \"there are\",set(len(v) for v in data_dict.values()), \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  21  samples in Enron Dataset.\n"
     ]
    }
   ],
   "source": [
    "my_dataset = data_dict\n",
    "print \"There are \", len(my_dataset['BUY RICHARD B']), \" samples in Enron Dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of people: 146\n"
     ]
    }
   ],
   "source": [
    "print('Total Number of people: %d' % len(data_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bonus deferral_payments deferred_income director_fees  \\\n",
      "0   600000               NaN             NaN           NaN   \n",
      "1  1200000           1295738        -1386055           NaN   \n",
      "2   350000               NaN         -400729           NaN   \n",
      "3      NaN               NaN             NaN           NaN   \n",
      "4  1500000               NaN        -3117011           NaN   \n",
      "\n",
      "              email_address exercised_stock_options expenses from_messages  \\\n",
      "0      mark.metts@enron.com                     NaN    94299            29   \n",
      "1                       NaN                 6680544    11200           NaN   \n",
      "2  steven.elliott@enron.com                 4890344    78552           NaN   \n",
      "3     bill.cordes@enron.com                  651850      NaN            12   \n",
      "4    kevin.hannon@enron.com                 5538001    34039            32   \n",
      "\n",
      "  from_poi_to_this_person from_this_person_to_poi        ...         \\\n",
      "0                      38                       1        ...          \n",
      "1                     NaN                     NaN        ...          \n",
      "2                     NaN                     NaN        ...          \n",
      "3                      10                       0        ...          \n",
      "4                      32                      21        ...          \n",
      "\n",
      "  long_term_incentive    other    poi restricted_stock  \\\n",
      "0                 NaN     1740  False           585062   \n",
      "1             1586055  2660303  False          3942714   \n",
      "2                 NaN    12961  False          1788391   \n",
      "3                 NaN      NaN  False           386335   \n",
      "4             1617011    11350   True           853064   \n",
      "\n",
      "  restricted_stock_deferred  salary shared_receipt_with_poi to_messages  \\\n",
      "0                       NaN  365788                     702         807   \n",
      "1                       NaN  267102                     NaN         NaN   \n",
      "2                       NaN  170941                     NaN         NaN   \n",
      "3                       NaN     NaN                      58         764   \n",
      "4                       NaN  243293                    1035        1045   \n",
      "\n",
      "  total_payments total_stock_value  \n",
      "0        1061827            585062  \n",
      "1        5634343          10623258  \n",
      "2         211725           6678735  \n",
      "3            NaN           1038185  \n",
      "4         288682           6391065  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "persons = pd.Series(list(data_dict.keys()))\n",
    "\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  104 Total index\n",
      "There are  101 travel agency in the park\n"
     ]
    }
   ],
   "source": [
    "total_index = data_dict.keys().index(\"TOTAL\")\n",
    "print \"There are \", (total_index), \"Total index\"\n",
    "travel_index = data_dict.keys().index(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "print \"There are \", (travel_index), \"travel agency in the park\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCKHART EUGENE E  {'salary': 'NaN', 'to_messages': 'NaN', 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 'NaN', 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': 'NaN', 'total_stock_value': 'NaN', 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 'NaN', 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}\n"
     ]
    }
   ],
   "source": [
    "print  \"LOCKHART EUGENE E \", my_dataset['LOCKHART EUGENE E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGBCAYAAABFHepEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XuYXXV97/H3lxBBUANCTbCNRQhBbC0wA0JUgggSDBal\nUHECUfHSA6LotF6fUw8H2opWLqKFakUFGplT1LaihEwMVbRCAGcArQKTBBC8EC7BIBIQku/5Y61J\ndsa57uw9s/bM+/U8+5nZv/Vba757ZSXzyW/91lqRmUiSJFXZdhNdgCRJ0kgMLJIkqfIMLJIkqfIM\nLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLCOIiMMi4uqI+EVEbIqI\n4+rYxoKIuDEiHouIByPiaxHxx82oV5KkycjAMrKdgduAdwNjfvBSROwJ/CewAtgfOBrYHfh6wyqU\nJGmSCx9+OHoRsQl4Y2ZeXdP2LODjwJuBXYAfAx/JzOvL5ScAV2bmDjXrvJ4ixOyQmRvH8SNIktSS\nHGHZdhcDhwBvAl4GfBW4NiL2Lpf3AJsi4tSI2C4iZgCLgW8bViRJGh1HWMZg4AhLRMwG7gZmZ+YD\nNf2+DdyUmX9bvp8PXAXsBkwDbgAWZuZj4/wRJElqSY6wbJuXUQSQvoj4Tf8LmA/sDRARM4EvAF8G\nDiqXPY1zWCRJGrXtJ7qAFvcc4BmgDdg0YNnj5dczgPWZ+dH+BRFxCnB/RLw8M28el0olSWphBpZt\ncyvFCMvMzPzBEH12ogg1tfrDjSNckiSNQsv9whzrfVEi4viIWF7e/2R9RNwQEUeP4eftHBH7R8QB\nZdNe5fvZmbkKuBK4ovw5e0bEyyPiIxHxurL/NcDLI+JjETEnItooTg/dQxF4JEnSCFousDD2+6LM\nB5YDr6M4dfMd4JsRsf8of95BFMGip/x55wO9wNnl8rcBVwDnAXcC/16ucx9AZn4HWAS8oVxvKbAB\neF1mPjXKGiRJmtJa+iqhwe6LMsr1/gf4f5n5982pTJIkNVIrjrBsk4gI4LnAuomuRZIkjc6UCyzA\nBylOK1010YVIkqTRmVJXCUXEIuBjwHGZ+fAw/XYDFgD3Ak+OT3WSJE0KOwJ7At2Z+UijNjplAktE\nvBn4F+DEciLscBYAX2l+VZIkTVonU1xJ2xBTIrBERAdwKXBSZi4bxSr3AixZsoT99tuvmaW1hM7O\nTi688MKJLmPCuR+2cF8U3A8F98MW7gu44447OOWUU6D8XdooLRdYImJnYA4QZdNe5SXK6zLz/og4\nF3hhZr617L8IuAw4E7ilvFU+wIZhnuXzJMB+++1HW1tbkz5J65gxY4b7AfdDLfdFwf1QcD9s4b7Y\nSkOnVLTipNuR7osyC5hd0/9dFHejvRj4Zc3r0+NUryRJ2kYtN8KSmdczTNDKzFMHvD+i6UVJkqSm\nasURFkmSNMUYWDSijo6OiS6hEtwPW7gvCu6HgvthC/dF87T0rfmbpXxAYU9PT4+TpyRJGoPe3l7a\n29sB2jOzt1HbdYRFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRV\nnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFF\nkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRVnoFFkiRV\nnoFFkiRVnoFFkiRVnoFFkiRVXssFlog4LCKujohfRMSmiDhuFOu8OiJ6IuLJiOiLiLeOR62SJKkx\nWi6wADsDtwHvBnKkzhGxJ/At4Dpgf+Ai4NKIeG3zSpQkSY20/UQXMFaZuQxYBhARMYpVTgfuzswP\nle/viohXAZ3At5tTpSRJaqRWHGEZq0OBFQPauoF5E1CLJEmqw1QILLOAtQPa1gLPi4gdJqAeSZI0\nRlMhsEiSpBbXcnNY6vAAMHNA20zgscx8argVOzs7mTFjxlZtHR0ddHR0NLZCSZJaUFdXF11dXVu1\nrV+/vik/KzJHvNCmsiJiE/DGzLx6mD6fAF6XmfvXtF0J7JKZC4dYpw3o6enpoa2trdFlS5I0afX2\n9tLe3g7Qnpm9jdpuy50SioidI2L/iDigbNqrfD+7XH5uRFxes8rnyj6fjIh9I+LdwInABeNcuiRJ\nY9bX18e1117LqlWrJrqUCdVygQU4CLgV6KG4D8v5QC9wdrl8FjC7v3Nm3gscCxxFcf+WTuAdmTnw\nyiFJkipj3bp1HHPMsey7774sXLiQuXPncswxx/Loo49OdGkTouXmsGTm9QwTtDLz1EHavge0N7Mu\nSZIaadGixaxYsRJYAswHvseKFWfS0XEKy5ZdM8HVjb+WCyySJE12fX19dHcvpQgrJ5etJ7NxY9Ld\nvZhVq1axzz77TGCF468VTwlJkjSprVmzpvxu/oAlhwOwevXqca2nCgwskiRVzN57711+970BS64H\nYM6cOeNaTxUYWCRJqpi5c+eyYMFCpk07k+K00P3AEqZNex8LFiyccqeDwMAiSVIldXUt4aijDgUW\nAy8CFnPUUYfS1bVkgiubGE66lSSpgnbddVeWLbuGVatWsXr1aubMmTMlR1b6GVgkSaqwffbZZ0oH\nlX6eEpIkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVn\nYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEk\nSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZVnYJEkSZXXkoElIs6I\niHsiYkNErIyIg0fof3JE3BYRv42IX0bEFyPi+eNVryRJ2jYtF1gi4iTgfOAs4EDgdqA7InYfov8r\ngcuBLwAvBU4EXg78y7gULEmStlnLBRagE/h8Zl6RmXcCpwFPAG8fov+hwD2ZeXFm/iwzbwA+TxFa\nJElSC2ipwBIR04F24Lr+tsxMYAUwb4jVbgRmR8Trym3MBP4SuKa51UqSpEZpqcAC7A5MA9YOaF8L\nzBpshXJE5RTg3yLid8CvgEeB9zSxTkmS1EDbT3QBzRYRLwUuAv4vsBzYAziP4rTQO4dbt7Ozkxkz\nZmzV1tHRQUdHR1NqlSSplXR1ddHV1bVV2/r165vys6I4o9IaylNCTwAnZObVNe2XATMy8/hB1rkC\n2DEz31TT9krg+8AemTlwtIaIaAN6enp6aGtra/wHkSRpkurt7aW9vR2gPTN7G7XdljollJlPAz3A\nkf1tERHl+xuGWG0n4JkBbZuABKIJZUqSpAZrqcBSugB4V0S8JSJeAnyOIpRcBhAR50bE5TX9vwmc\nEBGnRcSLy9GVi4CbMvOBca5dkiTVoeXmsGTmVeU9V84BZgK3AQsy86Gyyyxgdk3/yyPiOcAZFHNX\nfk1xldFHxrVwSZJUt5YLLACZeQlwyRDLTh2k7WLg4mbXJUmSmqMVTwlJkqQpxsAiSZIqz8AiSZIq\nz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8Ai\nSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIq\nz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8Ai\nSZIqz8AiSZIqryUDS0ScERH3RMSGiFgZEQeP0P9ZEfEPEXFvRDwZEXdHxNvGqVxJkrSNtp/oAsYq\nIk4Czgf+CrgZ6AS6I2JuZj48xGpfBf4AOBVYA+xBi4Y1SZKmopYLLBQB5fOZeQVARJwGHAu8HfjH\ngZ0j4hjgMGCvzPx12XzfONUqSZIaoKVGGSJiOtAOXNfflpkJrADmDbHanwM/BD4cET+PiLsi4lMR\nsWPTC5YkSQ3RaiMsuwPTgLUD2tcC+w6xzl4UIyxPAm8st/HPwPOBdzSnTEmS1EitFljqsR2wCViU\nmY8DRMRfA1+NiHdn5lNDrdjZ2cmMGTO2auvo6KCjo6OZ9UqS1BK6urro6uraqm39+vVN+VlRnFFp\nDeUpoSeAEzLz6pr2y4AZmXn8IOtcBrwiM+fWtL0E+AkwNzPXDLJOG9DT09NDW1tbwz+HJEmTVW9v\nL+3t7QDtmdnbqO221ByWzHwa6AGO7G+LiCjf3zDEaj8AXhgRO9W07Usx6vLzJpUqSZIaqKUCS+kC\n4F0R8ZZypORzwE7AZQARcW5EXF7T/0rgEeDLEbFfRMynuJroi8OdDpIkSdXRcnNYMvOqiNgdOAeY\nCdwGLMjMh8ous4DZNf1/GxGvBT4L3EIRXv4N+Ni4Fi5JkurWkMASEc8DXgPclZl3NGKbw8nMS4BL\nhlh26iBtfcCCZtclSZKao65TQhFxVUS8p/z+2RT3ObkK+FFEnNDA+iRJkuqewzIf+H75/fFAALsA\nZwJ/24C6JEmSNqs3sMwA1pXfHwN8PTOfAK4B9mlEYZIkSf3qDSz3A/MiYmeKwLK8bN+V4o6ykiRJ\nDVPvpNtPA18BHgd+Bny3bJ8P/Hjby5IkSdqirsCSmZdExM0Ulw9/OzM3lYvuxjkskiSpweq+rDkz\nf0hxdVBt2zXbXJEkSdIAdQWWiPjScMsz8+31lSNJkvT76h1h2XXA++nAn1Jc2vxf21SRJEnSAPXO\nYRnsqcjbAf8M/N7TjyVJkrZFwx5+WE68vQDobNQ2JUmSoPFPa96bFnygoiRJqrZ6J91eMLAJ2AM4\nFrh8W4uSJEmqVe9oyIED3m8CHgL+Bhj2CiJJkqSxqnfS7RGNLkSSJGkojZ7DIkmS1HB1BZaImBkR\n/xoRv4yIZyJiY+2r0UVKkqSprd45LJcBLwL+DvgVkI0qSJIkaaB6A8urgMMy87ZGFiNJkjSYeuew\n3E9xKbMkSVLT1RtY3g98IiL2bFwpkiRJg6v3lNC/ATsBayLiCeDp2oWZ+fxtLUySJKlfvYHl/Q2t\nQpIkaRj13jjO2+9LkqRxU/eDCiNiGvBGYL+y6SfA1ZnpfVgkSVJD1fvwwznAUuAPgbvK5o8C90fE\nsZm5pkH1SZIk1X2V0GeANcDszGzLzDaKG8ndUy6TJElqmHpPCR0OHJqZ6/obMvORiPgI8IOGVCZJ\nklSqd4TlKeC5g7Q/B/hd/eVIkiT9vnoDy7eAf4mIQ2KLQ4HPAVc3rjxJkqT6A8uZFHNYbgSeLF83\nAKuB9zWmNEmSpEK992H5NfCG8mqhl5bNP83M1Q2rTJIkqbQt92F5B9AJ7FM2rYqIT2fmpQ2pTJIk\nqVTXKaGIOAe4CPgm8Jfl65vAheWypoqIMyLinojYEBErI+LgUa73yoh4OiJ6m12jJElqnHpHWE4H\n3pWZXTVtV0fEj4DPAv9nmysbQkScBJwP/BVwM8UoT3dEzM3Mh4dZbwZwObACmNms+iRJUuPVO+l2\nOvDDQdp72IbTTKPUCXw+M6/IzDuB04AngLePsN7ngK8AK5tcnyRJarB6A8u/UoyyDPRXFKGgKSJi\nOtAOXNfflplJMWoyb5j1TgVeDJzdrNokSVLzjHo0JCIuqHmbwDsj4mi2jFgcQnF7/isaV97v2R2Y\nBqwd0L4W2HewFSJiH+DjwKsyc1NENLE8SZLUDGM5fXPggPc95de9y68Pl68/2daiGiUitqMY8Tmr\n5oGMJhZJklrMqANLZh7RzEJG6WFgI78/aXYm8MAg/Z8LHAQcEBEXl23bARERvwOOzszvDvXDOjs7\nmTFjxlZtHR0ddHR01Fe9JEmTSFdXF11dXVu1rV+/vik/K4opIK0jIlYCN2Xm+8r3AdwHfCYzPzWg\nbwD7DdjEGcARwAnAvZm5YZCf0Qb09PT00NbW1oRPIUnS5NTb20t7eztAe2Y27DYizb6ipxkuAC6L\niB62XNa8E3AZQEScC7wwM99aTsj9ae3KEfEg8GRm3jGuVUuSpLq1XGDJzKsiYnfgHIpTQbcBCzLz\nobLLLGD2RNUnSZIar+UCC0BmXgJcMsSyU0dY92y8vFmSpJZS731YJEmSxo2BRZIkVZ6BRZIkVZ6B\nRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIk\nVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6B\nRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIkVZ6BRZIk\nVZ6BRZIkVZ6BRZIkVV5LBpaIOCMi7omIDRGxMiIOHqbv8RGxPCIejIj1EXFDRBw9nvVKkqRt03KB\nJSJOAs4HzgIOBG4HuiNi9yFWmQ8sB14HtAHfAb4ZEfuPQ7mSJKkBWi6wAJ3A5zPzisy8EzgNeAJ4\n+2CdM7MzM8/LzJ7MXJOZ/xtYBfz5+JUsSZK2RUsFloiYDrQD1/W3ZWYCK4B5o9xGAM8F1jWjRkmS\n1HgtFViA3YFpwNoB7WuBWaPcxgeBnYGrGliXJElqou0nuoDxFBGLgI8Bx2XmwyP17+zsZMaMGVu1\ndXR00NHR0aQKJUlqHV1dXXR1dW3Vtn79+qb8rCjOqLSG8pTQE8AJmXl1TftlwIzMPH6Ydd8MXAqc\nmJnLRvg5bUBPT08PbW1tDaldkqSpoLe3l/b2doD2zOxt1HZb6pRQZj4N9ABH9reVc1KOBG4Yar2I\n6AC+CLx5pLAiSZKqpxVPCV0AXBYRPcDNFFcN7QRcBhAR5wIvzMy3lu8XlcvOBG6JiJnldjZk5mPj\nW7okSapHywWWzLyqvOfKOcBM4DZgQWY+VHaZBcyuWeVdFBN1Ly5f/S5niEuhJUlStbRcYAHIzEuA\nS4ZYduqA90eMS1GSJKlpWmoOiyRJmpoMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIk\nqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIMLJIkqfIM\nLJIkqfK2n+gC1Fr6+vpYs2YNc+bMYZ999pnociRJU4QjLBqVdevWccwxx7LvvvuycOFC5s6dyzHH\nHMujjz460aVJkqYAA4tGZdGixaxYsRJYAtwHLGHFipV0dJwyaP++vj6uvfZaVq1aNZ5lSpImKQOL\nRtTX10d391I2bvwMcDIwGziZjRsvort76VahxJEYSVIzGFg0ojVr1pTfzR+w5HAAVq9evbllrCMx\nkiSNhoFFI9p7773L7743YMn1AMyZMwcYOBJzMPA/wMsHHYmRJGksDCwa0dy5c1mwYCHbbXcG8EGK\n4LKEadPex4IFCzdfLbRlJOZLwL7AQmAu8GVg65EYSZLGwsCiEa1bt46nn36aTZvWA+dRnAp6K4cf\n3k5X15LN/YqRmO2AW6k9JVS8327zSIwkSWPlfVg0okWLFvPd795MMbryeuB+pk07k+nTp7PrrrsO\n6L0J+CzF5FzKrwksHrd6JUmTjyMsGtbNN99Md/eycnTlUxSjK1eycePHf29eylgm50qSNBYGFg3r\n9NPfAzyXrU/xrASuArYOIaOdnCtJ0lh5SkhD6uvro7f3FoqQMvgpnu2333II9U/OXbHiTDZuTIqR\nleuZNu19HHXUQm/lL0mqmyMsGtLgp3j6KOapAARHH30M8+cfsfnGcF1dSzjqqEMpAs2LgMUcddSh\nW03OlSRprBxh0ZC2PsXzOooQsrSmx57AOr7//Rvp6DiFZcuuYdddd2XZsmtYvnw5K1euZN68ebz2\nta8d58olSZONgUVDmjt3Lvvv38btt58BvJgtc1jmU4SYMyhODz21eQLubrvtxqJFi+nu3hJsFixY\nSFfXkkGuKJIkaXQ8JaRhPfvZOwJPALcBHy2/vxzYAPwt8Bj9h9Hq1au9Nb8kqSlacoQlIs4APgDM\nAm4H3puZtwzT/9XA+cCfUPwW/YfMvHwcSm1pfX19rFx5A/AsilDywZql21HMZen/CtOmTStHVrae\npLtxY9LdvZhVq1Y58VaSVJeWG2GJiJMowsdZwIEUgaU7InYfov+ewLeA64D9gYuASyPCiRUj2DLp\n9mlg+oCl04GXUlzyXASa++67r1zmfVgkSY3VcoEF6AQ+n5lXZOadwGkU5ynePkT/04G7M/NDmXlX\nZl4MfK3cjoawZs0aTjjhTUCUr53Y+l4sOwF3UpwW+h2wiU984lPl2t6HRZLUWC0VWCJiOtBOMVoC\nQGYmsAKYN8Rqh5bLa3UP01/AIYe8kg0bnqQYSam93f7s8utnyvYHN69zzz2/ZLfdZjJt2pkUoeZ+\nBntIoiRJY9VSgQXYHZgGrB3QvpZiPstgZg3R/3kRsUNjy5scuru7eeSRtcAzwMFl6+CneYozcoVN\nm87ikUfW8opXvAzvwyJJaqSWnHQ7Xjo7O5kxY8ZWbR0dHXR0dExQRePjpptuqnk3D/gBxWmek2va\nry+//jfFYXQ0cBLwQT760Q/zxS9+gdWrVzNnzhxHViRpkurq6qKrq2urtvXr1zflZ7VaYHkY2AjM\nHNA+E3hgiHUeGKL/Y5n51HA/7MILL6Stra2eOlvaIYccUvNuJsVAXP89V4rb7cN7yvYngYMoTgFd\nA7A5pBhUJGlyG+w/8b29vbS3tzf8Z7XUKaHMfBroAY7sb4uIKN/fMMRqN9b2Lx1dtmsQCxYsYLfd\nZlLk2b8H9gUep/Y0T3G6aEdgL4o5zNc4V0WS1DQtFVhKFwDvioi3RMRLgM9RXLJyGUBEnBsRtfdY\n+RywV0R8MiL2jYh3AyeW29EQbrnlRnbeeSfgN8AdbHl+UL/fst12zwCrca6KJKnZWi6wZOZVFDeN\nOwe4FfgzYEFmPlR2mUVxKUt//3uBY4GjKG7X2gm8IzMHXjmkGi9+8Yt5/PH1LF++jP3224/tt59G\nRLDDDjtw4IEHsnz5cjZufIq+vj6WLl1KX1/f5mcJSZLUaFFcFaxaEdEG9PT09EzJOSySJNWrZg5L\ne2b2Nmq7LTfCIkmSph4DiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwD\niyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJ\nqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwD\niyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqjwDiyRJqryWCiwRsWtEfCUi1kfEoxFxaUTsPEz/\n7SPikxHxo4h4PCJ+ERGXR8Qe41l3q+vq6proEirB/bCF+6Lgfii4H7ZwXzRPSwUW4EpgP+BI4Fhg\nPvD5YfrvBBwAnA0cCBwP7At8o7llTi7+BSy4H7ZwXxTcDwX3wxbui+bZfqILGK2IeAmwAGjPzFvL\ntvcC10TEBzLzgYHrZOZj5Tq123kPcFNE/FFm/nwcSpckSduolUZY5gGP9oeV0goggUPGsJ1dynV+\n3cDaJElSE7VSYJkFPFjbkJkbgXXlshFFxA7AJ4ArM/PxhlcoSZKaYsJPCUXEucCHh+mSFPNWtvXn\nbA98tdzeu0foviPAHXfcsa0/dlJYv349vb29E13GhHM/bOG+KLgfCu6HLdwXW/3u3LGR243MbOT2\nxl5AxG7AbiN0uxtYDJyXmZv7RsQ04EngxMwcciJtTVjZE3hNZj46Qk2LgK+M6gNIkqTBnJyZVzZq\nYxM+wpKZjwCPjNQvIm4EdomIA2vmsRwJBHDTMOv1h5W9gCNGCiulbuBk4F6KQCRJkkZnR4oBgu5G\nbnTCR1jGIiKWAi8ATgeeBXwJuDkzF9f0uRP4cGZ+owwrX6e4tPn1bD0HZl1mPj1uxUuSpLpN+AjL\nGC0C/oni6qBNwNeA9w3osw8wo/z+DymCCsBt5degmMdyBPC9ZhYrSZIao6VGWCRJ0tTUSpc1S5Kk\nKcrAIkmSKs/AUhrrgxXLdb4cEZsGvJaOV82NEBFnRMQ9EbEhIlZGxMEj9H91RPRExJMR0RcRbx2v\nWpttLPsiIg4f5M9+Y0S8YDxrbrSIOCwiri4fFLopIo4bxTqT7pgY636YxMfDRyPi5oh4LCLWRsR/\nRMTcUaw3GY+JMe+LyXhcRMRpEXF7+btyfUTcEBHHjLBOQ44HA8sWY32wYr9rgZkUd9udBXQ0q8BG\ni4iTgPOBsygeDnk70B0Ruw/Rf0/gW8B1wP7ARcClEfHa8ai3mca6L0pJMcm7/89+j8x8cJj+rWBn\nignq76b4fMOaxMfEmPZDaTIeD4cBn6V4/MlRwHRgeUQ8e6gVJvExMeZ9UZpsx8X9FDd7bQPagf8C\nvhERg97gtaHHQ2ZO+RfwEoqrjg6saVsAPAPMGma9LwP/PtH1b8PnXglcVPM+gJ8DHxqi/yeBHw1o\n6wKWTvRnmYB9cTiwEXjeRNfexH2yCThuhD6T9pgY436Y9MdD+Tl3L/fHq6byMTGGfTFVjotHgFOb\nfTw4wlLYlgcrvrocHrwzIi6JiOc3rcoGiojpFOn4uv62LI6kFRT7YzCHlstrdQ/TvyXUuS+gCDW3\nRcQvI2J5RLyiuZVW0qQ8Juo0FY6H/ofHrhumz1Q5JkazL2ASHxcRsV1EvBnYCbhxiG4NOx4MLIV6\nH6x4LfAW4DXAhyjS9NKIiCbV2Ui7A9OAtQPa1zL0Z541RP/nRfFgyVZVz774FfC/gBOAv6AYJv1u\nRBzQrCIrarIeE2M16Y+H8t+1TwP/nZk/HabrpD8mxrAvJuVxERF/GhG/AZ4CLgGOz8w7h+jesOOh\n1W4cNybR5AcrZuZVNW9/EhE/BtYArwa+U+92VX2Z2Qf01TStjIi9gU6g5ScYamymyPFwCfBS4JUT\nXUgFjGpV6JrWAAAGrklEQVRfTOLj4k6K+SgzgBOBKyJi/jChpSEmdWABzqOYZzKcu4EHKG75v1kU\nD1Z8frlsVDLznoh4GJhD9QPLwxTnVmcOaJ/J0J/5gSH6P5aZTzW2vHFVz74YzM1MvX/MJ+sx0QiT\n5niIiH8CFgKHZeavRug+qY+JMe6LwbT8cZGZz1D87gS4NSJeTnHX+dMH6d6w42FSnxLKzEcys2+E\n1zMU5952iYgDa1Yf8cGKA0XEH1E8ebqeg3hcZfEcpR6KzwlsHuY8ErhhiNVurO1fOpqhz122hDr3\nxWAOoAX+7BtsUh4TDTIpjofyF/QbKB4ee98oVpm0x0Qd+2Iwk+K4GGA7YKjTO407HiZ6dnFVXsBS\n4IfAwRTp9y7gXwf0uRN4Q/n9zsA/UkzK/ePyD+SHwB3A9In+PKP8zG8CnqCYh/MSisu4HwH+oFx+\nLnB5Tf89gd9QzPrel+KSz98BR030Z5mAffE+4Dhgb+BPKM5nPw28eqI/yzbuh50phnoPoLgC4v3l\n+9lT6ZioYz9M1uPhEuBRikt6Z9a8dqzp8/EpckzUsy8m3XFRfsbDyt97f1r+XXgGeE25vGn/Rkz4\nh6/Ki2LG9xJgfXlQfgHYaUCfjcBbyu93BJZRDHc9STE89s/9v+Ba5VUePPcCGygS70E1y74M/NeA\n/vMpRiM2AKuAxRP9GSZiXwAfLD//b4GHKK4wmj/Rn6EB++Dw8hf0xgGvL02lY2Ks+2ESHw+D7YPN\n/w5OsWNizPtiMh4XwKXl77sN5e+/5ZRhpdnHgw8/lCRJlTep57BIkqTJwcAiSZIqz8AiSZIqz8Ai\nSZIqz8AiSZIqz8AiSZIqz8AiSZIqz8AiSdIUExGHRcTVEfGLiNgUEceNcf2zyvU2ll/7X79pVs0G\nFkmSpp6dgdso7vBdzx1kPwXMAvYov84Cfgpc1agCBzKwSKq8ev4HKGlombksM/9PZn6D4kG/W4mI\nZ0XEeRHx84h4PCJujIjDa9Z/IjMf7H9RBJeXAl9sVs0GFkmSNNDFFA/3fRPwMuCrwLURsfcQ/d8J\n3JWZY3nC/ZgYWCRNehExfaJrkFpFRMwG3gb8ZWbekJn3ZOYFwA+AUwfpvwOwiOLBiE1jYJE0LiLi\nxIj4UUQ8EREPR8TyiHh2RBxUfv9QRPw6Ir4bEQeOsK1PRMRdEfHbiFgTEedExLSa5WdFxK0R8Y6I\nuBvYEBGLy587fcC2/jMiLm/Sx5Za0cuAaUBfRPym/0Xx1OXBRlj+AngOcEUzi9q+mRuXJICImAVc\nCXwA+E/gucBhFOfOnwtcBpxB8Z+ovwGWRsSczPztEJt8DHgL8CuKf1y/ULadV9NnDsU/pMcDG4HV\nwEXAccDXy7r+AFgIHNWYTypNCs8BngHagE0Dlj0+SP93AN/KzIeaWZSBRdJ42IPif2z/kZn3l20/\nKb9+p7ZjRJwGnAQcDiwdbGOZ+fGat/dFxPnlOrWBZTqwODPX1Wy7i2JI++tl02LgZ5n5vXo+lDRJ\n3Urx93VmZv5guI4RsSdwBPD6ZhdlYJE0Hm4HrgP+JyK6geXA1zLz1xHxAuAfKALKCyj+oXw28KKh\nNhYRJwHvpRiefg7Fv2XrB3T7WW1YKX0BuDki9sjMXwFvBb68rR9OajURsTPFKGT/FUJ7RcT+wLrM\nXBURVwJXRMQHKALMC4DXALdn5rU1m3oH8EtgWbNrdg6LpKbLzE2ZeTRwDMXIynuBO8v/nV0B/FnZ\nNg/YH1gHPGuwbUXEPGAJ8C3gWOAAisAzsP/vnU7KzNuAHwFviYg2isswnb+iqeggiiDSQ3EflvOB\nXuDscvnbKP5ungfcCfx7uc59/RuIiKAM/ZlZz71cxsQRFknjJjNvBG6MiL8DfkYxv+QVwOmZ2Q2b\nr1DYfZjNzAPuzcxP9DeUwWe0LgXeD/wRsCIzfzGWzyBNBpl5PcMMWmTmRorwcvYwfZJhRkIbzcAi\nqeki4uXAkRSngh4EDqUIJT8F+oDFEdEDzAD+EXhimM2tAl5Unha6heLc+RvHUM6VFP9rfCfFHBZJ\nLcBTQpLGw2MUl0ReA9wFnAP8dTmq8k5gV4qh6cspruR5cMD6m4ebM/ObwIXAZymGtA8ttzcqmfkY\nxaTbx4Fv1PdxJI23GIfTTpJUKRGxAvhxZnZOdC2SRsdTQpKmjIjYheISzMOB0ye4HEljYGCRNJXc\nCuwCfCgzV010MZJGz1NCkiSp8px0K0mSKs/AIkmSKs/AIkmSKs/AIkmSKs/AIkmSKs/AIkmSKs/A\nIkmSKs/AIkmSKu//Ay3eylzzODsHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10691c110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "features = [\"salary\", \"bonus\"]\n",
    "#data_dict.pop('TOTAL', 0)\n",
    "data = featureFormat(data_dict, features)\n",
    "### plot features\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter( salary, bonus )\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "101\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_imp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3685331a540a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtravel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"THE TRAVEL AGENCY IN THE PARK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtravel_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_imp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_imp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtravel_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_imp' is not defined"
     ]
    }
   ],
   "source": [
    "total_index = my_dataset.keys().index(\"TOTAL\")\n",
    "print(total_index)\n",
    "travel_index = my_dataset.keys().index(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "print(travel_index)\n",
    "df_subset = df_imp.drop(df_imp.index[[total_index,travel_index]])\n",
    "df_subset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### remove any outliers before proceeding further\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data_dict.pop('TOTAL', 0)\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "### remove NAN's from dataset\n",
    "outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['salary']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    outliers.append((key, int(val)))\n",
    "\n",
    "outliers_final = (sorted(outliers,key=lambda x:x[1],reverse=True)[:4])\n",
    "### print top 4 salaries\n",
    "print outliers_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def dict_to_list(key,normalizer):\n",
    "    new_list=[]\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n",
    "            new_list.append(0.)\n",
    "        elif data_dict[i][key]>=0:\n",
    "            new_list.append(float(data_dict[i][key])/float(data_dict[i][normalizer]))\n",
    "    return new_list\n",
    "\n",
    "### create two lists of new features\n",
    "poi_sender_fract=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\n",
    "poi_recipient_fract=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\n",
    "\n",
    "### insert new features into data_dict\n",
    "count=0\n",
    "for i in data_dict:\n",
    "    data_dict[i][\"poi_sender_fract\"]=poi_sender_fract[count]\n",
    "    data_dict[i][\"poi_recipient_fract\"]=poi_recipient_fract[count]\n",
    "    count +=1\n",
    "\n",
    "    \n",
    "features_list = [\"poi\", \"poi_sender_fract\", \"poi_recipient_fract\"]    \n",
    "    ### store to my_dataset for easy export below\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "### these two lines extract the features specified in features_list\n",
    "### and extract them from data_dict, returning a numpy array\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "\n",
    "### plot new features\n",
    "for point in data:\n",
    "    from_poi = point[1]\n",
    "    to_poi = point[2]\n",
    "    plt.scatter( from_poi, to_poi )\n",
    "    if point[0] == 1:\n",
    "        plt.scatter(from_poi, to_poi, color=\"r\", marker=\"*\")\n",
    "plt.xlabel(\"fraction of emails this person gets from poi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_array = []\n",
    "for x in data_dict.values():\n",
    "    if x['email_address'] == 'NaN':\n",
    "        email_array.append(x['email_address'])\n",
    "        \n",
    "print len(email_array), \"has email address\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 'have a quantified salary from this dataset')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NA_salary = []\n",
    "for x in data_dict.values():\n",
    "    if x['salary'] != 'NaN':\n",
    "        NA_salary.append(x['salary'])\n",
    "len(NA_salary),\"have a quantified salary from this dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: key  poi_sender_fract  not present\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0c030b62bb63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m### feature in the array is the label, which is why \"poi\" must always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m### be first in features_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m### split data into training and testing datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-aeb05f68b15a>\u001b[0m in \u001b[0;36mtargetFeatureSplit\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "features_list = [\"poi\", \"salary\", \"bonus\", \"poi_sender_fract\", \"poi_recipient_fract\",\n",
    "                 'deferral_payments', 'total_payments', 'loan_advances', 'restricted_stock_deferred',\n",
    "                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options',\n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\n",
    "data = featureFormat(data_dict, features_list)\n",
    "\n",
    "### split into labels and features (this line assumes that the first\n",
    "### feature in the array is the label, which is why \"poi\" must always\n",
    "### be first in features_list\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### split data into training and testing datasets\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "pred= clf.predict(features_test)\n",
    "print 'accuracy', score\n",
    "\n",
    "print \"Decision tree algorithm time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "import numpy as np\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print 'Feature Ranking: '\n",
    "for i in range(16):\n",
    "    print \"{} feature {} ({})\".format(i+1,features_list[i+1],importances[indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 8 features I pick would be:\n",
    "[\"salary\", \"bonus\", \"fraction_from_poi_email\", \"fraction_to_poi_email\", 'deferral_payments', 'total_payments', 'loan_advances', 'restricted_stock_deferred']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ea2a1f7982e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "features_list = [\"poi\", \"poi_sender_fract\", \"poi_recipient_fract\", \"shared_receipt_with_poi\"]\n",
    "### try Naive Bayes for prediction\n",
    "t0 = time()\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred,labels_test)\n",
    "print accuracy\n",
    "\n",
    "print \"NB algorithm time:\", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d0432c0380b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### use manual tuning parameter min_samples_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "### use manual tuning parameter min_samples_split\n",
    "clf = DecisionTreeClassifier(min_samples_split=5)\n",
    "print clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: key  poi_sender_fract  not present\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-66bfd2452909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m### feature in the array is the label, which is why \"poi\" must always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m### be first in features_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-aeb05f68b15a>\u001b[0m in \u001b[0;36mtargetFeatureSplit\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "### features_list is a list of strings, each of which is a feature name\n",
    "### first feature must be \"poi\", as this will be singled out as the label\n",
    "#features_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\", 'shared_receipt_with_poi']\n",
    "features_list = [\"poi\", \"poi_sender_fract\", \"poi_recipient_fract\", \"shared_receipt_with_poi\"]\n",
    "\n",
    "### store to my_dataset for easy export below\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "### these two lines extract the features specified in features_list\n",
    "### and extract them from data_dict, returning a numpy array\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "\n",
    "\n",
    "### split into labels and features (this line assumes that the first\n",
    "### feature in the array is the label, which is why \"poi\" must always\n",
    "### be first in features_list\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "### machine learning goes here!\n",
    "### please name your classifier clf for easy export below\n",
    "\n",
    "### deploying feature selection\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "### use KFold for split and validate algorithm\n",
    "from sklearn.cross_validation import KFold\n",
    "kf=KFold(len(labels),3)\n",
    "for train_indices, test_indices in kf:\n",
    "    #make training and testing sets\n",
    "    features_train= [features[ii] for ii in train_indices]\n",
    "    features_test= [features[ii] for ii in test_indices]\n",
    "    labels_train=[labels[ii] for ii in train_indices]\n",
    "    labels_test=[labels[ii] for ii in test_indices]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "print 'accuracy before tuning ', score\n",
    "\n",
    "print \"Decision tree algorithm time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "\n",
    "### use manual tuning parameter min_samples_split\n",
    "t0 = time()\n",
    "clf = DecisionTreeClassifier(min_samples_split=5)\n",
    "clf = clf.fit(features_train,labels_train)\n",
    "pred= clf.predict(features_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "acc=accuracy_score(labels_test, pred)\n",
    "\n",
    "print \"Validating algorithm:\"\n",
    "print \"accuracy after tuning = \", acc\n",
    "\n",
    "# function for calculation ratio of true positives\n",
    "# out of all positives (true + false)\n",
    "print 'precision = ', precision_score(labels_test,pred)\n",
    "\n",
    "# function for calculation ratio of true positives\n",
    "# out of true positives and false negatives\n",
    "print 'recall = ', recall_score(labels_test,pred)\n",
    "\n",
    "\n",
    "### dump your classifier, dataset and features_list so\n",
    "### anyone can run/check your results\n",
    "pickle.dump(clf, open(\"my_classifier.pkl\", \"w\") )\n",
    "pickle.dump(data_dict, open(\"my_dataset.pkl\", \"w\") )\n",
    "pickle.dump(features_list, open(\"my_feature_list.pkl\", \"w\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "Got a divide by zero when trying out: GaussianNB()\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    #Added min max scaler here\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    features = min_max_scaler.fit_transform(features)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_positives += 1\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    pickle.dump(clf, open(CLF_PICKLE_FILENAME, \"w\") )\n",
    "    pickle.dump(dataset, open(DATASET_PICKLE_FILENAME, \"w\") )\n",
    "    pickle.dump(feature_list, open(FEATURE_LIST_FILENAME, \"w\") )\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    clf = pickle.load(open(CLF_PICKLE_FILENAME, \"r\") )\n",
    "    dataset = pickle.load(open(DATASET_PICKLE_FILENAME, \"r\") )\n",
    "    feature_list = pickle.load(open(FEATURE_LIST_FILENAME, \"r\"))\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-9fb5becf4219>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-9fb5becf4219>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ref:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ref: \n",
    "    https://cs224d.stanford.edu/reports/Mahendra.pdf\n",
    "    https://www.kaggle.com/forums/f/15/kaggle-forum/t/12611/methodological-ml-dispute-with-a-udacity-teacher-could-you-help-me-understanding\n",
    "    http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
